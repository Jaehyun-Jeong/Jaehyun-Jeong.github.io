---
title: A Practical Summary of the Lux AI season 3 #1 Teamâ€™s Approach
tags: RL Kaggle
article_header:
  type: cover
pseudocode: true
---

## Intro

* Why I started to learn this.

## Quick introduction of Lux Ai Season3

* Game Explanation
    * observation
    * action
    * goal

## #1 Team's Approach

### Observations

### Model Architecture

At first glance, their model was such a disaster to understand. However, calculating data flow shape through out the model always leads me to understanding of model. I hope the other people like this sort of method because this is a really powerful method to handle complex models. Now, I'm going to explain how #1 team's model works in this manner showing data shape flow.

First off, their model is separated into four parts, data embedding, general spatial processing, prediction, and action selection. So, let's see how they've made the architecture.
```
Embedding -> ResNet -> ConvLSTM -> Transformer -> base_out |$\cdot$| = (4, 64, 24, 24)$
```

#### Embedding

**Data Embedding** is a crucial part when there are a number of data types such as spatial data(grid information), binary data(fog or asteroids mask), and discrete data(score and units' energy). Observation space of Lux ai's gym is huge so that they've picked more than 1000 features from the gym. But an important point of the choosing methology is that they combined them into 3 dimensional space. This has a significant impact because they can be processed with convolution layers. They also merged these information with covolution layers by shrinking channels or using embedding layer. Codes below are #1 team's embeddings.

```python
class ConvEmbeddingInputLayer(nn.Module):
    def __init__(
            self,
            obs_space: gym.spaces.Dict,
            embedding_dim: int,
            out_dim: int,
            n_merge_layers: int = 1,
            activation: Callable = nn.LeakyReLU,
            obs_space_prefix: str = ""
    ):
        super(ConvEmbeddingInputLayer, self).__init__()

        self.obs_space_prefix = obs_space_prefix
        self.embedding_dim = embedding_dim
        self.out_dim = out_dim

        # Embedding for discrete features
        self.embedding = nn.Embedding(
            num_embeddings=875,  # Total unique discrete values
            embedding_dim=embedding_dim
        )

        n_discrete_features = 72
        n_continuous_features = 74

        # Merge layers: Conv2D to merge continuous + embedded discrete features
        self.merge_layers = nn.Sequential(
            *[
                 nn.Conv2d(
                     embedding_dim * n_discrete_features + n_continuous_features + 3,
                     out_dim,
                     (1, 1)
                 ),
                 activation()
             ] * n_merge_layers
        )

    def forward(self, x, prev_prediction) -> torch.Tensor:
        # x is a dictionary with keys 'continues_features' and 'discrete_features'
        continuous = x[self.obs_space_prefix + "GPU1_continues_features"]  # |B, 1, P, C_continuous, H, W|
        discrete = x[self.obs_space_prefix + "GPU1_discrete_features"]    # |B, 1, P, C_discrete, H, W|

        B, _, P, C_discrete, H, W = discrete.shape

        # Move P to batch dimension for discrete features
        discrete = discrete.view(B * P, C_discrete, H, W)  # |B * P, C_discrete, H, W|

        # Apply embedding
        discrete_flat = discrete.view(B * P, -1).contiguous().to(dtype=torch.int32)  # |B * P, C_discrete * H * W|
        embedded = self.embedding(discrete_flat)  # |B * P, C_discrete * H * W, embedding_dim|

        # Reshape back to spatial dimensions
        embedded = embedded.view(B * P, C_discrete, H, W, self.embedding_dim)  # |B * P, C_discrete, H, W, embedding_dim|
        embedded = embedded.permute(0, 4, 1, 2, 3).contiguous()  # |B * P, embedding_dim, C_discrete, H, W|
        embedded = embedded.view(B * P, -1, H, W)  # Flatten embedding dimension: |B * P, C_discrete * embedding_dim, H, W|

        # Move P to batch dimension for continuous features
        continuous_flat = continuous.view(B * P, -1, H, W)  # |B * P, C_continuous, H, W|

        prev_prediction_flat = prev_prediction.view(B * P, -1, H, W)
        prev_prediction_flat = torch.sigmoid(prev_prediction_flat)

        # Merge continuous and embedded discrete features
        combined = torch.cat([continuous_flat, embedded, prev_prediction_flat], dim=1)  # |B * P, C_continuous + C_discrete * embedding_dim, H, W|

        # Apply merge layers
        output = self.merge_layers(combined)  # |B * P, out_dim, H, W|

        return output
```

ConvEmbeddingInputLayer class gets **discrete, continuous features, and previous prediction** as inputs.

* Discrete Channels
    * Embed with the Embedding layer

* Continuous Channels
    * Do nothing before combined

* Previous Prediction Channals
    * preprocess with sigmoid

* Combined
    * Combine discrete, continuous and previous prediction with "torch.cat"
    * Merge every channel with convolution layer

```python
class BinaryInputLayer(nn.Module):
    def __init__(
            self,
            num_classes: int = 875,  # The total unique discrete values
            out_dim: int = 64,
            n_merge_layers: int = 1,
            activation: Callable = nn.LeakyReLU,
            obs_space_prefix = ""
    ):
        super(BinaryInputLayer, self).__init__()

        self.obs_space_prefix = obs_space_prefix

        self.num_classes = num_classes  # 875 unique discrete values
        self.out_dim = out_dim

        n_continuous_features = 74  # Continuous feature count

        # Merge layers: Conv2D to merge continuous + one-hot discrete features
        self.merge_layers = nn.Sequential(
            nn.Conv2d(
                num_classes + n_continuous_features + 3,  # 3 = prev_prediction channels
                num_classes // 2,
                (1, 1)
            ),
            activation(),
            nn.Conv2d(
                num_classes // 2,  # 3 = prev_prediction channels
                out_dim,
                (1, 1)
            ),
            activation()
        )

    def forward(self, x, prev_prediction, one_player=None) -> torch.Tensor:
        """
        x: dictionary with keys 'GPU1_continues_features' and 'GPU1_discrete_features'
        - Continuous shape: [B, 1, P, C_continuous, H, W]
        - Discrete shape: [B, 1, P, C_discrete, H, W]  (Each pixel has C_discrete values, but they don't overlap)
        """

        continuous = x[self.obs_space_prefix + "GPU1_continues_features"]  # |B, 1, P, C_continuous, H, W|
        output = x[self.obs_space_prefix + "GPU1_one_hot_encoded_discrete_features"]

        if one_player is not None:
            continuous = continuous[:, :, one_player, ...].unsqueeze(2)
            output = output[:, :, one_player, ...].unsqueeze(2)

        B, _, P, _, H, W = output.shape

        # Move P to batch dimension for continuous features
        continuous_flat = continuous.view(B * P, -1, H, W)  # |B * P, C_continuous, H, W|
        output = output.view(B * P, -1, H, W)

        # Process previous predictions
        prev_prediction_flat = prev_prediction.view(B * P, -1, H, W)
        prev_prediction_flat = torch.sigmoid(prev_prediction_flat)

        # Merge continuous, one-hot discrete, and prev_prediction features
        output = torch.cat([output, continuous_flat, prev_prediction_flat], dim=1)  # |B * P, total_channels, H, W|

        # Apply merge layers
        output = self.merge_layers(output)  # [B * P, out_dim, H, W]

        return output
```

### Learning Algorithm

### Future goal

## Questions

### How can an agent choose action in the IMPALA setup?

### How can a model combine a various types of inputs?
